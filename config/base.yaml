
# Trainig paradigm
rl_or_sft: "rl"
# System settings
system:
  cuda_visible_devices: 0
  n_gpus: 1
  vllm_attention_backend: "XFORMERS"
  hydra_output_subdir: "outputs/exp_configs/logs/$(date +%Y-%m-%d)/$(date +%H-%M-%S)"

# Model settings
model:
  base_model: "Qwen/Qwen2.5-0.5B-Instruct"
  experiment_name: "ragen-main-exp"
  gradient_checkpointing: true

# Training parameters
training:
  train_data_num: null
  val_data_num: 100
  val_batch_size: 100
  micro_batch_size: 4
  train_batch_size: 8
  ppo_batch_size: 32
  max_start_length: 400
  max_response_length: 400
  max_obs_length: 200
  max_turns: 5
  rollout_tp_size: 1
  n_rollout: 16
  total_epochs: 5
  temperature: 0.7
  use_sft: false
  no_think_rl: false
  state_masking: false
  binary_reward: false
  mask_state: false
  length_penalty: false
  ref_update_steps: null # every ref_update_steps, the reference model will be updated using the latest actor model
  total_training_steps: 200

# Optimization parameters
optimization:
  actor_lr: 1e-6
  critic_lr: 1e-5
  use_kl_loss: true # this means actor KL Loss
  kl_coef: 0.01
  kl_type: low_var_kl
  adv_estimator: gae
  gpu_memory_utilization: 0.4
  no_ref_policy: false
  betas: [0.9,0.95]
  reward_norm_type: brpo

# Logging settings
logging:
  mode: "['wandb']"
  log_images: true
  log_image_dir: "log/trajectory"
  log_image_step_size: 4
  log_n_image_per_batch: 32

# Trainer settings
trainer:
  val_before_train: true
  val_only: false
  default_hdfs_dir: null
  nnodes: 1
  save_freq: 200
  test_freq: 50 # 100
  project_name: "RAGEN"

env_interface:
  config:
    envs_size: null # same as batch size
    envs_type: ["sokoban", "frozen_lake", "bandit", "countdown"]
    env_configs:
      sokoban:
        dim_x: 6
        dim_y: 6
        
# SFT settings
sft:
  env_type: "sokoban"  # or "frozenlake"
  output_dir: "outputs/sft"

  # Data generation parameters
  data_generation:
    data_dir: "data/sft"
    algo: "bfs"
    seed: 100000 # needs to be different from the seed in the RL config
    train_size: 1000
    test_size: 100
    bfs_max_depths: 100
    prefix: "message"
    num_processes: 16

  # Training parameters
  training:
    num_gpus: 1
    max_length: 2048
    learning_rate: 1e-4
    train_batch_size: 128
    micro_batch_size: 4
    experiment_name: "test_sft_lora"
    logger: "['console','wandb']"
    epochs: 5
    hdfs_dir: null
    validate_before_training: true
    lora_rank: 64
    lora_alpha: 32
    target_modules: "all-linear"
    enable_gradient_checkpointing: false
    base_model: "Qwen/Qwen2.5-0.5B-Instruct"
    project_name: "RAGEN"
  
  # Sokoban-specific settings
  sokoban:
    dim_x: 6
    dim_y: 6
    num_boxes: 1
    max_steps: 100
    search_depth: 30
  
  # FrozenLake-specific settings
  frozenlake:
    size: 4
    p: 0.8
    is_slippery: true
